# Final Project Report: Hybrid Search System & Sentiment Analysis (Phase 3)

## 1. Project Overview
This final phase integrates the two primary search strategies developed in previous steps—**Lexical Search** and **Semantic Search**—into a unified **Hybrid Search** engine. Additionally, a **Sentiment Analysis** module has been implemented to classify user reviews as either positive or negative, providing deeper insights into user attitudes toward products.

---

## 2. System Architecture
The system is built as a modular pipeline connecting the following components:
* **Preprocessing Module:** Uses the `Hazm` library for Persian text normalization and lemmatization.
* **Lexical Engine:** Powered by **Elasticsearch**, utilizing the **BM25** algorithm for precise keyword matching and handling typos.
* **Semantic Engine:** Utilizes **ParsBERT** for generating high-dimensional embeddings and **Qdrant** for vector similarity search.
* **Fusion Layer:** Merges results from both engines using the **Reciprocal Rank Fusion (RRF)** algorithm.
* **Sentiment Classifier:** A **Logistic Regression** model trained on BERT-extracted features to predict review polarity.



---

## 3. Hybrid Search Implementation (RRF)
A major challenge in hybrid retrieval is merging scores from different scales (e.g., Elasticsearch’s unbounded BM25 scores vs. Qdrant’s cosine similarity). To address this, we implemented the **Reciprocal Rank Fusion (RRF)** algorithm.

### Algorithm Logic:
RRF ignores raw scores and focuses solely on the **rank** of each document in the respective result lists. The final score for a document $d$ is calculated as:
$$Score_{RRF}(d) = \sum_{r \in \{Lexical, Semantic\}} \frac{1}{k + rank_r(d)}$$

* **Smoothing Constant ($k$):** Set to **60** to moderate the influence of top-ranked documents and ensure a balanced merge.
* **Outcome:** This ensures that documents appearing consistently high in both lists are prioritized, effectively combining keyword precision with contextual relevance.

---

## 4. Sentiment Analysis Module

### Data Preparation
To ensure high-quality training data:
* **Negative Class (0):** Ratings of 1 and 2.
* **Positive Class (1):** Ratings of 4 and 5.
* **Neutral Removal:** Ratings of **3** were excluded to eliminate ambiguity and sharpen the decision boundaries of the classifier.

### Strategy Selection: Strategy 1 (Feature Extraction)
We selected **Strategy 1** for the sentiment classification task.
* **Methodology:** We utilized the 768-dimensional embeddings generated by **ParsBERT** as input features ($X$) for a **Logistic Regression** classifier.
* **Justification:**
    1. **Efficiency:** This approach is computationally lightweight and does not require expensive GPU resources for training.
    2. **Semantic Richness:** ParsBERT's pre-trained embeddings already capture deep contextual and emotional nuances of the Persian language.
    3. **Stability:** By keeping BERT's weights frozen (Feature Extraction), we avoid overfitting on smaller datasets while maintaining high generalization capabilities.

---

## 5. Evaluation & Performance
The sentiment model was evaluated on a held-out test set using the following metrics:
* **Accuracy:** The overall ratio of correct predictions.
* **F1-Score:** The harmonic mean of Precision and Recall, ensuring a balanced performance evaluation.
* **Confusion Matrix:** Used to analyze False Positives and False Negatives to understand model behavior on polarized reviews.



---

## 6. Scenario Analysis (Comparison)
To demonstrate the superiority of the Hybrid approach, we analyzed challenging queries (e.g., *"Poor camera quality at night"*). 

The results (detailed in the `result.txt` file) show:
1. **Lexical Results:** High precision for specific keywords but often fails to capture the underlying topic if synonyms are used.
2. **Semantic Results:** Excellent at capturing the concept of "low-light photography" but might miss specific keyword constraints.
3. **Hybrid (RRF) Results:** Provides the best of both worlds by ranking relevant semantic matches that also contain key search terms at the top.

---

## 7. Project Structure
* `main.py`: Coordinates the end-to-end execution of search and sentiment tasks.
* `preprocess.py`: Contains normalization and lemmatization logic.
* `embeddings.py`: Handles ParsBERT model loading and vector generation.
* `vector_db.py`: Manages the Qdrant vector database connection.
* `search_engines.py`: Implements Elasticsearch queries and RRF fusion.
* `sentiment.py`: Implements the Sentiment Analysis training and prediction logic.
* `result.txt`: Contains execution logs, search comparison lists, and the final evaluation report.
